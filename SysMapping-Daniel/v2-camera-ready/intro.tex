The emergence of new architectures like the cloud opens new opportunities for data integration. 
The possibility of having unlimited access to cloud resources and the ``pay as U go'' model make it possible to change the hypothesis for processing big  data collections.  Instead of designing processes and algorithms taking into consideration  limitations on resources availability, the cloud sets the focus on the economic cost implied when using resources and producing results.

 
Integrating and processing heterogeneous huge data collections (i.e., Big Data) calls for efficient methods for correlating, associating, and filtering them according to their ``structural'' characteristics (due to data variety) and their quality (veracity), e.g., trust, freshness, provenance, partial or total consistency. 
Existing data integration techniques must be revisited considering weakly curated and modeled data sets provided by different services under different quality conditions. Data integration can be done according to  (i) quality of service (QoS) requirements expressed by their consumers and (ii) Service Level Agreements (SLA)  exported by the cloud providers that host  huge data collections and deliver resources for executing the associated management processes. 
Yet, it is not an easy task to completely enforce SLAs particularly because
consumers use several cloud providers to store, integrate and process the data
they require under the specific conditions they expect. For example, a major concern when
integrating data from different sources (services) is privacy that can be
associated to the conditions in which integrated data collections are built and
shared~\cite{YauY08}.     
Naturally, a collaboration between cloud providers becomes necessary~\cite{036}
but this should be  done in a user-friendly way, with some degree of
transparency. 

In this context, the main contribution of our work is
 a classification scheme of existing works fully or partially addressing
the problem of integrating data in multi-cloud environments taking into
consideration an extended form of SLA. 
The classification scheme results from  applying the  methodology defined
in~\cite{SM:Petersen:2008} called  \textit{systematic mapping}. It consists of dimensions clustered
into facets in which publications (i.e., papers) are aggregated according to
frequencies (i.e., number of published papers). According to the methodology,
the study consists in  five interdependent steps including (i) the definition of
a research scope by defining research questions; (ii) retrieving candidate
papers by querying different scientific databases (e.g. IEEE, Citeseer, DBLP);
(iii) selecting relevant papers that can be used for answering the research
questions by defining inclusion and exclusion criteria; (iv) defining a
classification scheme by analyzing the abstracts of the selected papers to
identify the terms to be used as dimensions for classifying the papers;
(v) producing a systematic mapping by sorting papers according to the
classification scheme.              

The remainder of this paper is organized as follows. Section~\ref{sec:sm}
describes our study of data integration perspectives and the evolution of the
research works that address some aspects of the problem. Section~\ref{sec:qanalysis} gives a quantitative
analysis of our study and identifies open issues in the field.
Section~\ref{sec:conc} concludes the paper and discusses future work with
reference to the stated problem.


