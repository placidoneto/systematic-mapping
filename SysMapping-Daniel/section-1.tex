\section{Introduction}
\label{sec:intro}

The emergence of new architectures like the cloud opens new opportunities to data processing. 
The possibility of having unlimited access to cloud resources and the ``pay as U go'' model make it possible to change the hypothesis for processing big  data collections.  Instead of designing processes and algorithms taking into consideration  limitations on resources availability, the cloud sets the focus on the economic cost implied of using resources and producing results by parallelizing their use while delivering data under subscription oriented cost models.
 
Integrating and processing heterogeneous Big Data, calls for efficient methods for correlating, associating, filtering them taking into consideration their ``structural'' characteristics (due to variety) but also their quality (veracity), e.g., trust, freshness, provenance, partial or total consistency. 
Existing data integration techniques have to be revisited considering weakly curated and modeled data sets. This can be done according to quality of service requirements expressed by their consumers and Service Level Agreement (SLA) contracts exported by the cloud providers that host  Big Data and deliver resources for executing the associated management processes. Yet, it is not an easy task to completely fulfill   SLA contracts particularly because they have to use several cloud providers to integrate the data they require under the conditions they expect.
Naturally, a collaboration between cloud providers becomes necessary~\cite{036} but this should be done in a user-friendly way, with high degree of transparency. 

To better understand, let us consider an example from the domain of energy
management. We assume we are interested in queries like: \textit{Give a list
of energy providers that can provision 1000 KW-h, in the next 10 seconds, that are close to my city, with a cost of 0,50 Euro/KW-h and that are labelled as green?} We consider a simplified SLA cloud contract inspired in the cheapest contract provided by Azure: \textit{cost of \$0,05 cents per call,  8~GB of I/O volume/month, free data transfer cost within the same region,  1~GB of storage.} 
The user is ready to pay a maximum of \textit{\$5 as total query cost}; she requests that only  \textit{green} energy providers should be  listed (provenance), with at least  \textit{85$\%$} of precision of provided data, even if they are not fresh; she requires an availability rate of at least 90$\%$ and a response time of  \textit{0,01 s}. 
  The question is how can the user efficiently obtain  results for her queries such that they meet her QoS requirements, they respect her subscribed contracts with the involved cloud provider(s) and such that they do not neglect services contracts? Particularly, for queries that call several services deployed  on different clouds.

%\subsection{Contribution}
The contribution of this paper is two-fold. First it proposes a classification scheme of existing works fully or partially addressing the problem of integrating data in multi-cloud environments taking into consideration Service Level Agreement. The classification scheme results from  applying the  methodology defined in~\cite{SM:Petersen:2008} called  systematic mapping  that enables to build a classification of the field. The classification consists of categories grouped into facets  to group and aggregate  publications according to frequencies. The study consists in  five interdependent tasks including (i) the definition of a research scope by defining research questions; (ii) retrieving candidate papers by querying different scientific databases (e.g. IEEE, Citeseer, DBLP); (iii) selecting relevant papers that can be used for answering the research questions by defining inclusion and exclusion criteria; (iv) defining a classification scheme by  analyzing the abstracts of selected papers to identify the terms that will be used as categories for classifying the papers; (v) producing a systematic mapping by sorting papers according to the classification scheme. Our final objective by applying the systematic mapping methodology is to identify trends and open issues regarding our research topic and proposing an approach that fills some gaps and proposes an original data integration solution according to current trends in the area. Thus, our systematic mapping consists in three facets that classify existing scientific publications addressing  together or independently SLA, Data Integration in Multi-cloud environments. It shows the research trends of data integration as a result of the emergence of the cloud and the characteristics associated to Big Data that require ressources in order to be processed.


%\subsection{Organization of the paper}
The remainder of this paper is organized as follows. 
Section~\ref{sec:rw} discusses existing approaches studying data integration problems in multi-cloud contexts that take into account SLA contacts and that are supported by cloud providers for providing efficient solutions when the data collections they deal with are close to some Big Data properties: volume, variety, velocity, veracity, etc.
Section~\ref{sec:sm} describes our study of  data integration perspectives and the evolution of the research works that address some aspects of the problem. The Section gives a quantitative analysis of our study and identifies open issues in the field. Section \ref{sec:approach} gives the general lines of the approach we propose for guiding data integration using SLA agreements in a multi-cloud environment. We show initial experiments that show the feasibility of our approach.  
Section~\ref{sec:conc} concludes the paper and discusses future work. 


